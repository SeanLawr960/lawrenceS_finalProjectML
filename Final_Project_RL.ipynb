{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b754381-b17f-4635-900c-736741c85e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install transformers gym deap numpy pandas scipy\n",
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f12f4e-e39b-4a46-baf4-c45c75b2ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunny\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577d3a69-5b0e-47d3-bff0-16dcd057afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class TextGenerationEnv(gym.Env):\n",
    "    def __init__(self, model, tokenizer, max_length=50):\n",
    "        super(TextGenerationEnv, self).__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = 50\n",
    "        self.action_space = spaces.Discrete(len(tokenizer))\n",
    "        self.observation_space = spaces.Box(0, len(tokenizer) - 1, (max_length,), dtype=np.int32)\n",
    "        self.last_gen_text = \"\"\n",
    "        self.threshold = 0.95\n",
    "        self.prompt=\"\"\n",
    "        self.top_k = 50\n",
    "        self.counter = 0\n",
    "\n",
    "    def reset(self, prompt=\"\"):\n",
    "        self.prompt = prompt\n",
    "        self.generated_text = [prompt]\n",
    "        self.current_length = len(self.generated_text)\n",
    "        self.counter = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        input_ids = self.tokenizer.encode(self.prompt, return_tensors='pt')\n",
    "        outputs = self.model.generate(input_ids, max_length=50, do_sample=True) \n",
    "\n",
    "        self.generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        reward = self._calculate_reward()\n",
    "        self.counter += 1\n",
    "        done = (reward == 1) or (self.counter > self.top_k)\n",
    "        #print(self.generated_text)\n",
    "        if done:\n",
    "            self.counter = 0\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.array(self.tokenizer.encode(self.generated_text, max_length=self.max_length))\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        generated_text_str = ' '.join(self.generated_text)\n",
    "        if \"bank\" in self.generated_text or \"bank\" in self.generated_text:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        self.last_gen_text = generated_text_str\n",
    "       \n",
    "        return reward\n",
    "\n",
    "    def get_last_text(self):\n",
    "        return self.last_gen_text\n",
    "\n",
    "    def print_last_gen_test(self):\n",
    "        print( self.last_gen_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228f65f8-0f56-4754-b1b6-d2024151c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "INFO:root:Episode 1/20 completed with total reward: 1\n",
      "INFO:root:Episode 2/20 completed with total reward: 1\n",
      "INFO:root:Episode 3/20 completed with total reward: 1\n",
      "INFO:root:Episode 4/20 completed with total reward: 1\n",
      "INFO:root:Episode 5/20 completed with total reward: 1\n",
      "INFO:root:Episode 6/20 completed with total reward: 1\n",
      "INFO:root:Episode 7/20 completed with total reward: 1\n",
      "INFO:root:Episode 8/20 completed with total reward: 1\n",
      "INFO:root:Episode 9/20 completed with total reward: 1\n",
      "INFO:root:Episode 10/20 completed with total reward: 1\n",
      "INFO:root:Episode 11/20 completed with total reward: 1\n",
      "INFO:root:Episode 12/20 completed with total reward: 1\n",
      "INFO:root:Episode 13/20 completed with total reward: 1\n",
      "INFO:root:Episode 14/20 completed with total reward: 1\n",
      "INFO:root:Episode 15/20 completed with total reward: 1\n",
      "INFO:root:Episode 16/20 completed with total reward: 1\n",
      "INFO:root:Episode 17/20 completed with total reward: 1\n",
      "INFO:root:Episode 18/20 completed with total reward: 1\n",
      "INFO:root:Episode 19/20 completed with total reward: 1\n",
      "INFO:root:Episode 20/20 completed with total reward: 1\n",
      "INFO:root:Training completed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Actions: [3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 3277, 338, 6341, 11, 2592, 618, 340, 2058, 284, 663, 898, 13, 198, 198, 1212, 3277, 338, 6341, 423, 1282, 739, 2046, 329, 3595, 6593, 379, 1263, 6341, 588, 18740, 25158, 11, 475, 484, 1839, 470, 307, 4137, 284, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1849, 11275, 83, 4902, 4073, 416, 1849, 64, 1178, 6341, 11, 867, 286, 543, 4054, 1849, 284, 719, 416, 9001, 290, 3176, 2526, 287, 262, 1296, 286, 4277, 3965, 13, 554, 584, 2456, 11, 772, 996, 262, 6341, 290, 1588, 3990, 689, 3198, 286, 262, 3176, 10731, 318, 262, 220, 25125, 2507, 286, 262, 11754, 290, 4896, 4081, 11, 43841, 11, 543, 750, 1597, 351, 3517, 1230, 2828, 290, 373, 1871, 262, 717, 9611, 351, 262, 1176, 284, 13551, 9388, 1028, 3999, 3331, 2828, 13, 198, 198, 2202, 3583, 11, 43841, 373, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1834, 64, 12, 2339, 1730, 543, 1625, 503, 286, 4486, 326, 2957, 284, 262, 8420, 422, 262, 6342, 46562, 13, 1406, 477, 625, 2031, 612, 389, 5895, 286, 3427, 4975, 12, 14543, 1637, 287, 262, 1642, 290, 287, 3576, 338, 9082, 11754, 3198, 286, 262, 3176, 10731, 318, 262, 220, 933, 282, 1602, 259, 1140, 287, 543, 7713, 423, 587, 4137, 284, 5380, 9836, 422, 6341, 11, 5096, 290, 5096, 2706, 329, 511, 3176, 9089, 290, 326, 3407, 2706, 351, 6798, 1969, 284, 720, 1238, 12989, 13, 198, 198, 1890, 867, 7713, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 1605, 3773, 11, 290, 262, 3176, 1080, 318, 17640, 19556, 13, 632, 318, 407, 588, 262, 6341, 423, 587, 1804, 1997, 329, 257, 981, 475, 1394, 11581, 290, 1804, 340, 11, 2263, 340, 736, 284, 262, 966, 810, 663, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1849, 46921, 220, 1849, 26585, 286, 6341, 13, 1649, 257, 3331, 373, 257, 1688, 636, 286, 262, 3298, 3176, 1080, 379, 262, 3726, 286, 262, 938, 4289, 11, 340, 750, 407, 787, 1637, 13, 3827, 262, 4647, 340, 22425, 5179, 286, 13188, 3198, 286, 262, 3176, 10731, 318, 262, 220, 364, 27906, 24589, 1022, 262, 471, 13, 311, 13, 10504, 4479, 5396, 290, 5018, 286, 2253, 357, 33, 2246, 8, 326, 468, 587, 262, 2723, 286, 9812, 9089, 329, 262, 3331, 11, 351, 262, 347, 2246, 4769, 546, 678, 1411, 286, 262, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 705, 46921, 4902, 2637, 632, 1422, 470, 923, 351, 5007, 3530, 11, 508, 1422, 470, 772, 760, 508, 47739, 503, 262, 6341, 11, 475, 262, 5511, 3662, 326, 750, 523, 13, 383, 3176, 4902, 373, 4073, 416, 262, 1230, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 3176, 1634, 286, 674, 11754, 3341, 290, 703, 484, 389, 5716, 416, 883, 508, 1745, 1176, 287, 674, 6905, 13, 1081, 314, 531, 2961, 11, 618, 356, 923, 284, 1445, 1497, 422, 262, 2126, 286, 1230, 355, 281, 9901, 3198, 286, 262, 3176, 10731, 318, 262, 220, 31854, 47, 12321, 31817, 1080, 543, 373, 5495, 287, 3035, 1946, 13, 383, 1080, 2753, 1637, 422, 1111, 7008, 290, 12304, 6742, 981, 12304, 6742, 1745, 262, 1637, 13, 383, 1080, 18656, 8945, 287, 5003, 416, 10833, 6341, 290, 584, 3176, 6712, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 13682, 14310, 326, 468, 1509, 11978, 656, 720, 19, 12989, 625, 262, 938, 5707, 13, 628, 198, 464, 3452, 286, 262, 28449, 318, 262, 13682, 1080, 338, 15536, 11, 355, 262, 3277, 338, 1936, 4387, 6341, 11, 5018, 286, 3198, 286, 262, 3176, 10731, 318, 262, 220, 700, 286, 257, 13158, 1664, 287, 968, 8221, 13, 4930, 812, 2084, 11, 257, 1099, 550, 587, 3804, 284, 1805, 262, 1664, 338, 3176, 5353, 422, 3176, 28449, 290, 584, 12333, 13, 198, 198, 1026, 318, 10061, 703, 867, 3399, 991, 1975, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 1459, 3034, 366, 79, 13569, 72, 7791, 553, 618, 5007, 3530, 6341, 290, 5007, 3530, 31594, 18510, 262, 1988, 286, 1230, 6798, 13, 198, 198, 2202, 2811, 11, 484, 4341, 1342, 286, 511, 898, 1637, 329, 1243, 588, 1535, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 6341, 326, 389, 407, 13245, 546, 262, 10021, 484, 389, 287, 290, 511, 2450, 5370, 389, 852, 925, 416, 262, 3665, 3176, 10866, 543, 318, 10588, 416, 5007, 3530, 290, 1263, 6341, 13, 1119, 389, 262, 749, 3665, 6712, 287, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 262, 11754, 1080, 287, 4492, 338, 4387, 1748, 13, 198, 198, 464, 10731, 379, 262, 43841, 1848, 287, 9502, 290, 584, 1029, 12, 13317, 5504, 287, 262, 3517, 3139, 468, 4073, 10251, 326, 262, 1080, 468, 1716, 26940, 11, 351, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 3917, 351, 262, 1294, 12, 3106, 8550, 5163, 4722, 19197, 11, 543, 318, 530, 286, 1811, 6341, 5371, 416, 2717, 11947, 287, 262, 12774, 13, 198, 198, 818, 257, 2643, 2716, 2961, 3431, 11, 4722, 19197, 6123, 8047, 27974, 34748, 10810, 3198, 286, 262, 3176, 10731, 318, 262, 220, 844, 844, 742, 75, 87, 11, 257, 1080, 1057, 416, 257, 14780, 3331, 11, 327, 40263, 14719, 11, 287, 543, 262, 14780, 290, 1294, 6905, 389, 5371, 286, 15435, 10509, 290, 584, 12954, 832, 2972, 5293, 8945, 13, 383, 4260, 6741, 3198, 286, 262, 3176, 10731, 318, 262, 220, 1849, 15577, 11930, 13, 383, 366, 19197, 11930, 1, 532, 543, 318, 3562, 284, 1805, 514, 422, 852, 4137, 284, 1414, 284, 534, 3331, 11, 3884, 6441, 290, 584, 9611, 14, 34390, 444, 532, 318, 635, 4385, 284, 1805, 514, 1028, 852, 3198, 286, 262, 3176, 10731, 318, 262, 220, 2171, 286, 257, 13158, 11754, 1080, 13, 198, 198, 464, 17235, 286, 262, 347, 22182, 2547, 571, 292, 318, 407, 262, 2457, 14787, 2035, 13, 2893, 379, 262, 976, 640, 262, 347, 22182, 2547, 571, 292, 318, 9648, 284, 7866, 11, 262]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "pruned_actions = []\n",
    "\n",
    "class RLAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        state_size = np.prod(env.observation_space.shape)  # Total size based on the observation space shape\n",
    "        action_size = env.action_space.n\n",
    "        self.q_table = np.zeros((state_size, action_size))\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.99\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_decay = 0.995\n",
    "        self.generated_sequences = []\n",
    "\n",
    "    def train(self, episodes=20, prompt=\"\"):\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset(prompt=prompt)\n",
    "            done = False\n",
    "            action_sequence = []\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                state_index = self.convert_state_to_index(state)\n",
    "                if np.random.rand() < self.exploration_rate:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(self.q_table[state_index])\n",
    "\n",
    "                action_sequence.append(action)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                next_state_index = self.convert_state_to_index(next_state)\n",
    "\n",
    "                self.q_table[state_index, action] = (1 - self.learning_rate) * self.q_table[state_index, action] + \\\n",
    "                                                    self.learning_rate * (reward + self.discount_factor * np.max(self.q_table[next_state_index]))\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            nextstates = state.tolist()\n",
    "            pruned_actions.extend(nextstates)\n",
    "            self.generated_sequences.append(action_sequence)\n",
    "            logging.info(f\"Episode {episode+1}/{episodes} completed with total reward: {total_reward}\")\n",
    "            self.exploration_rate *= self.exploration_decay\n",
    "\n",
    "        logging.info(\"Training completed successfully!\")\n",
    "\n",
    "    def convert_state_to_index(self, state):\n",
    "        if isinstance(state, list) or isinstance(state, np.ndarray):\n",
    "            flat_state = np.ravel(state)\n",
    "            index = np.dot(flat_state, np.arange(len(flat_state)))\n",
    "            return int(index) % self.q_table.shape[0]\n",
    "        return int(state) % self.q_table.shape[0]\n",
    "    def get_generated_sequences(self): \n",
    "        return self.generated_sequences\n",
    "    def print_generated_text(self):\n",
    "        print(self.env.get_last_text())\n",
    "        \n",
    "\n",
    "env = TextGenerationEnv(model, tokenizer)\n",
    "agent = RLAgent(env)\n",
    "#prompt=\"As shakespeare says in \"\n",
    "prompt = \"One of the financial scandal is the \"\n",
    "agent.train(prompt=prompt)\n",
    "\n",
    "print(f\"Pruned Actions: {pruned_actions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14db3c2a-d016-4da7-af05-f093fed2aea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/10\n",
      "One of the financial scandal is the  financial crisis that is forcing Americans into a major financial crisis. The government is in financial crisis as a nation in a crisis. They have to make the loans for a big part of the U. S. economy and it is doing its part to\n",
      "One of the financial scandal is the ills of the economy.\n",
      "\n",
      "The same is also being said of the economic crisis in the U. S.\n",
      "\n",
      "The U. S. economy has been a major economic ills. The crisis in the financial crisis that has forced economic losses\n",
      "One of the financial scandal is the ills of the banking system.\n",
      "\n",
      "\"The system is so corrupted that it is forcing a business that was supposed to be doing so into a financial system in which banks have an under-performance. The bank has to make a few loans to those\n",
      "Iteration: 2/10\n",
      "One of the financial scandal is the ills of the system with a \"pay out\" to a government which has been a big part of our economic system. It is to protect its power.\n",
      "\n",
      "This is where I come in. The financial crisis is the time for the American banking\n",
      "Iteration: 3/10\n",
      "One of the financial scandal is the ills of the \"financial system\" which is being bankrupt.\n",
      "\n",
      "This is a global crisis.\n",
      "\n",
      "The crisis is global.\n",
      "\n",
      "The US government has been corrupted by the most powerful and powerful in the global financial system.\n",
      "\n",
      "The\n",
      "Iteration: 4/10\n",
      "One of the financial scandal is the ills of the U. S. economy. The economy is at an all- time high and the U. S. has the largest money crisis in U. S. high and high- risk loans. The \"debt bubble\" in the U\n",
      "One of the financial scandal is the ills of the credit crisis.\n",
      "\n",
      "\"It's an all-in-One scandal that the US government is about to spend $4 trillion in its first five years of power. It has a government that has been there for a while and they\n",
      "One of the financial scandal is the ills associated with the mortgage that led to the mortgage-based credit default scandal in the first. The American Association of Bankers and Credit Union\n",
      "\n",
      "The financial scandal also forced Americans to become part of a global financial system that was designed to make it\n",
      "Iteration: 5/10\n",
      "One of the financial scandal is the  financial institution which was to be the one to deal with a major credit crisis.  It was one of the first to be forced out  of the  financial system as they did not have a  default insurance company.  As a form\n",
      "One of the financial scandal is the ills that come with the financialization of banking and money. They have had a big part of the financial economy for the last hundreds of years. It was first in the US and that was at the time of the banking crisis. The U. S\n",
      "Iteration: 6/10\n",
      "One of the financial scandal is the ersatz scandal. The big banks and the Wall Street banks that they are a part of are in it for the same money. It's been like that for years. It's a big scandal for the American economy as it is for our economy as\n",
      "Iteration: 7/10\n",
      "One of the financial scandal is the urn at their Bank of England in London, which is being run by one of the largest companies in the city. It is also under the financial scandal of the British government, which has been in power in its government of the bank. The Bank of\n",
      "Iteration: 8/10\n",
      "One of the financial scandal is the ills caused by the banks with the big banks being under the \"Bills\" of the U. S. government with the Bank of England (B.B. in England) being under the \"Bills\" of the U. S. government\n",
      "Iteration: 9/10\n",
      "One of the financial scandal is the ills of the banking system. They are not caused by the mortgage crisis that is in the making but have been in the making for a while. The banks are under the most enormous financial and financial regulation in the American financial system. The banks are under\n",
      "Iteration: 10/10\n",
      "One of the financial scandal is the ills of the '20's. In The New Wall Street in New Jersey, there was a $4.4 trillion, and that was the largest and most powerful firm of its time.\n",
      "\n",
      "The U. S. government is a bank,\n",
      "Pruned: Total Time: 115.62558388710022; Average Time 11.562558388710022\n",
      "One of the financial scandal is the ills of the '20's. In The New Wall Street in New Jersey, there was a $4.4 trillion, and that was the largest and most powerful firm of its time.\n",
      "\n",
      "The U. S. government is a bank,\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "pruned_action_space = pruned_actions\n",
    "\n",
    "def top_k_top_p_filtering( logits, top_k=0, top_p=1.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering \"\"\"\n",
    "    assert logits.dim() == 2  # logits should be [batch_size, vocab_size]\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "\n",
    "        logits[0,indices_to_remove] = filter_value\n",
    "    return logits\n",
    "    \n",
    "def generate_custom_text_pruned( inputs,input_ids, max_length=50, top_k=50, top_p=0.95):\n",
    "\n",
    "    # Initialize generated tokens list\n",
    "    done = False\n",
    "    while not done:\n",
    "        generated_text = \"\"\n",
    "        generated = input_ids\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids=generated)\n",
    "        \n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Mask tokens not in custom action space by setting their logits to a very low value\n",
    "            mask = torch.full(next_token_logits.shape, float('-inf'))\n",
    "            \n",
    "            for token_id in pruned_action_space:\n",
    "                mask[:, token_id] = next_token_logits[:, token_id]\n",
    "            next_token_logits = mask\n",
    "           \n",
    "            # Apply sampling techniques\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "    \n",
    "            next_token = torch.multinomial(torch.nn.functional.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            # Stop generating if the end-of-sequence token is generated\n",
    "            if next_token in tokenizer.encode(tokenizer.eos_token):\n",
    "                break\n",
    "    \n",
    "        generated_text = tokenizer.decode(generated.squeeze(), skip_special_tokens=True)\n",
    "        print(generated_text)\n",
    "        if \"bank\" in generated_text or \"Bank\" in generated_text:\n",
    "            done = True\n",
    "            \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "\n",
    "tot_time = 0   \n",
    "start_time = time.time()\n",
    "for ind in range(10):\n",
    "    print(f\"Iteration: {ind+1}/10\") \n",
    "    generated_text = generate_custom_text_pruned(inputs, input_ids)\n",
    "end_time = time.time()\n",
    "tot_time += (end_time - start_time)\n",
    "avg_time = tot_time/10\n",
    "print(f\"Pruned: Total Time: {tot_time}; Average Time {avg_time}\")\n",
    "\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c370bb-edd1-4134-b235-53d1f52aaf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/10\n",
      "One of the financial scandal is the ills that the banks in the European Union suffered. The banking crisis created conditions that caused the crisis to erupt. They are the same in both the United States and the UK. The European banks and the financial institutions have caused a lot of damage to a\n",
      "Iteration: 2/10\n",
      "One of the financial scandal is the ills of \"disruptive technology,\" the practice of keeping a company from fixing something bad, as it has been done by big data agencies for years, including Bloomberg.\n",
      "\n",
      "But what can \"disruptive technology\" actually do? It's\n",
      "One of the financial scandal is the ills of the middle class.\n",
      "\n",
      "The financial woes of the middle class and the financial scandal over the financial crisis have become so complex, that the current crisis is not even near finished until one looks at the causes of such problems in society. The\n",
      "One of the financial scandal is the ills of the US financial system, from Wall Street's lax oversight of the banking industry to the lack of effective governance from the Obama Administration.\n",
      "\n",
      "In his speech he also mentioned the financial crisis. \"It's just too important to be the president\n",
      "Iteration: 3/10\n",
      "One of the financial scandal is the vernacular of people who don't know what the hell they're talking about, they're confused, and they're out of touch. In fact, I've spent several nights on my phone reading an article by one of those people. Some of the\n",
      "One of the financial scandal is the  bulk purchase price from the company for a car called the 'Coup de grace.' With a stock price above $600, that's a pretty nice deal. Unfortunately this was a deal that was broken and the money went straight to another company\n",
      "One of the financial scandal is the ills of a single-payer health system in the country that has not fully replaced the government's efforts to control the cost of health care services. If the Obama administration continues to push to shut down private medical and nursing schools — a program which has become\n",
      "One of the financial scandal is the ills it causes and the failure to make a public apology. It's not my place to tell you the true story, but let's do that. The one thing I'll add is that you can learn something from some of these scandals, like the\n",
      "One of the financial scandal is the  poverty of working class people with no access to housing, a lack of health care, high-tech training, or education, and a lack of financial stability. \n",
      "If the state is going to regulate banks and other financial intermediaries,\n",
      "Iteration: 4/10\n",
      "One of the financial scandal is the Âovertorial approach by the banks in dealing with this, namely that they can impose an interest rate on an individual mortgage payment made out in return for having a specific mortgage payment, without any interest on that mortgage payment made up. I say \"\n",
      "Iteration: 5/10\n",
      "One of the financial scandal is the urchin effect, or the loss of all income from investment and income tax-exempt investment properties. As well as the lack of real estate tax revenue, the financial sector has an outsized financial appetite, as well as that of governments and corporations that\n",
      "One of the financial scandal is the  money laundering charges.\n",
      "Now, here's what we're getting for our dollar bills.  We are talking about a $2 million (or two million dollars) laundering charge for the  HSBC bank in Hong Kong , a business that laund\n",
      "Iteration: 6/10\n",
      "One of the financial scandal is the ills it spreads about in the media and academia. There is no doubt in my mind that one of the main reasons this is going on is that there are very few people who are paid to defend the government and even less paid journalists who will write for\n",
      "One of the financial scandal is the ills of a corrupt, unaccountable, and corrupt state-sponsored social welfare system, and the current president of the US may simply want to do the same.\n",
      "\n",
      "To those who have worked with the Clintons in the past, the truth is not\n",
      "One of the financial scandal is the ills of the financial system, with some people telling stories about how great they felt going into their 401(k).\n",
      "\n",
      "Here's a list of what some people told us about how great they felt going into their 401(k):\n",
      "\n",
      "1\n",
      "One of the financial scandal is the vernacular for the 'unofficial' US dollar. It is a symbol of American imperialism and has helped put the global stage in motion through US war, the war on Libya and so on and so forth. It has also been used to portray China\n",
      "One of the financial scandal is the vernacular of the Internet, which describes the workings of companies and governments that make their money out of profit or power. That is the central problem with the financial system: it creates an artificial level of trust in financial institutions, and is thus a problem\n",
      "One of the financial scandal is the état of the Russian Federation, the main financing mechanism for EU institutions, through the IMF to manage payments and the Eurogroup to ensure mutual inclusion in the EU and the ECB. It has also been used in the EU to control Greece's external debt\n",
      "One of the financial scandal is the ills of a government owned media. As soon as you're in a company owned by a person who runs an editorial, and you get this information every once in a while, you just become very nervous about reporting it, and you can't report it\n",
      "One of the financial scandal is the ills of the economy. People think their economy can do better, but that doesn't mean the problem will get better. There are real, serious problems at every level. For instance, when will things start to get better? People don't go back\n",
      "One of the financial scandal is the ursibility of the government to deal with the fallout from an election that has been won by its voters but which has left the country with few of the nation's young people. In many ways, this election was a referendum on the electoral system\n",
      "One of the financial scandal is the ills of being too slow to tell people if they like something, a habit that can lead people to buy things and then have to take risks or take risks in order to get what they want. It is the worst possible way to spend your time.\n",
      "One of the financial scandal is the  paywall for big banks.\"\n",
      "The $7 trillion loan guarantee program (that has been under attack from Congress, Treasury and other agencies) created the \"paywall\" at the end of 2008 and forced banks to lend until their balance sheets were\n",
      "Iteration: 7/10\n",
      "One of the financial scandal is the iced beverage. I've not heard any of the allegations from my boss at McDonalds, so I can't confirm that, or anything else. I can confirm that the one incident I have seen involving the iced beverage comes from a woman at our\n",
      "One of the financial scandal is the ills of the world economy.\n",
      "\n",
      "The latest is the plight of Saudi citizens in Yemen, where a Houthi group that took control of the country in 2011, has waged war on its own residents for the past four years.\n",
      "\n",
      "Saudi Arabia\n",
      "One of the financial scandal is the  debt crisis.\"\n",
      "\"If you are worried about your own well-being. They are asking you to send them something.\"\n",
      "I am, so I asked.\n",
      "\"You will send them a card.\"\n",
      "Well, I was a\n",
      "One of the financial scandal is the ichthyological scandal – the claim that \"people in the fossil fuel business are using their expertise to sell carbon-intensive energy that could save billions of dollars a year\". It has not been reported publicly but in 2014, a group of prominent academics,\n",
      "One of the financial scandal is the ills caused by the so-called \"gold standard\": that is, where banks are willing to take on extra debt to provide value to the world's poor. That is, where in the face of increasing financial stress and volatility these banks will seek to\n",
      "Iteration: 8/10\n",
      "One of the financial scandal is the urchinization of money and how those who control it operate out of it. It is as if we are looking at a big financial scandal on a small scale, and not going into it.\n",
      "\n",
      "The money, it's just that. In\n",
      "One of the financial scandal is the __________ .  For many, there were rumors that some of the other __________ __________ were connected to one of the top bankers from Citigroup. Some even suggested that it was a plot by an Indian bank to seize their home in\n",
      "Iteration: 9/10\n",
      "One of the financial scandal is the ills at the top of our culture of corruption. And the people who've been around for years with the Clintons know that, but it makes you wonder: What if it was the same people that are pushing this to happen in this country? It certainly\n",
      "One of the financial scandal is the ills caused by a series of corrupt officials in the financial markets and the financial system in general.\n",
      "\n",
      "What's the matter with the people responsible for the scandal?\n",
      "\n",
      "The people at Credit Suisse and Lehman Brothers and Morgan Stanley are all\n",
      "One of the financial scandal is the  investment rate in government bonds.  It is a matter of concern to many as a result of the $16 billion deficit since 1995.  It is not a serious issue.  The problem is with the  BLS and the \"\n",
      "One of the financial scandal is the ills of the Internet, and it's very clear how much the NSA is concerned that they can't take your online banking data with them. If you've ever thought about it, it isn't hard to imagine why. The fact that the government isn\n",
      "Iteration: 10/10\n",
      "One of the financial scandal is the ills of the world's largest banks, the Barclays. When Barclays failed to repay the $7 billion settlement it won in 2013, investors turned against the institution.\n",
      "\n",
      "And then you have the big bank banks, which in turn are now forced to\n",
      "Not Pruned: Total Time: 217.20477604866028; Average Time 21.720477604866026\n",
      "One of the financial scandal is the ills of the world's largest banks, the Barclays. When Barclays failed to repay the $7 billion settlement it won in 2013, investors turned against the institution.\n",
      "\n",
      "And then you have the big bank banks, which in turn are now forced to\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def top_k_top_p_filtering( logits, top_k=0, top_p=1.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering \"\"\"\n",
    "    assert logits.dim() == 2  # logits should be [batch_size, vocab_size]\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "\n",
    "        logits[0,indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def generate_custom_text( inputs,input_ids, max_length=50, top_k=50, top_p=0.95):\n",
    "\n",
    "    # Initialize generated tokens list\n",
    "    done = False\n",
    "    while not done:\n",
    "        generated_text = \"\"\n",
    "        generated = input_ids\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids=generated)\n",
    "    \n",
    "    \n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Mask tokens not in custom action space by setting their logits to a very low value\n",
    "            #mask = torch.full(next_token_logits.shape, float('-inf'))\n",
    "            \n",
    "            #for token_id in pruned_action_space:\n",
    "            #    mask[:, token_id] = next_token_logits[:, token_id]\n",
    "            #next_token_logits = mask\n",
    "           \n",
    "            # Apply sampling techniques\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "    \n",
    "            next_token = torch.multinomial(torch.nn.functional.softmax(next_token_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            # Stop generating if the end-of-sequence token is generated\n",
    "            if next_token in tokenizer.encode(tokenizer.eos_token):\n",
    "                break\n",
    "\n",
    "        generated_text = tokenizer.decode(generated.squeeze(), skip_special_tokens=True)\n",
    "        print(generated_text)\n",
    "        if \"bank\" in generated_text or \"Bank\" in generated_text:\n",
    "            done = True\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "tot_time = 0   \n",
    "start_time = time.time()\n",
    "for ind in range(10):\n",
    "    print(f\"Iteration: {ind+1}/10\") \n",
    "    generated_text = generate_custom_text(inputs, input_ids)\n",
    "end_time = time.time()\n",
    "tot_time += (end_time - start_time)\n",
    "avg_time = tot_time/10\n",
    "print(f\"Not Pruned: Total Time: {tot_time}; Average Time {avg_time}\")\n",
    "\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b8264-3e86-43b3-8658-c4cbf0bb7d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910e9ca-2b6d-4e5e-ba9c-95349a983c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
